{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import glob\n",
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import pdb;\n",
        "import torchvision\n",
        "import torch\n",
        "import nibabel as nib\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import models\n",
        "import glob\n",
        "import csv\n",
        "import copy\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "#import torch\n",
        "import numpy as np\n",
        "import torch.utils.data as data\n",
        "import torch.utils\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import click\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from torch.utils import data\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision import models\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import recall_score\n",
        "\n"
      ],
      "metadata": {
        "id": "rj4a3tl2MwjO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iov1LdnCiXWP",
        "outputId": "3524a48d-c65b-447a-d61a-b5f7a64d2e88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.1.0+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->torchvision) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU8B5LIa9krI",
        "outputId": "3c2094ae-fd33-417a-9eec-9c188020016c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.device = torch.device(\"cuda:0\")\n",
        "        PATH = '/content/drive/MyDrive/Science fair 22-23//Model_log/weights.pt'\n",
        "        self.fmodel = torch.load(PATH, map_location=self.device)\n",
        "        self.fmodel = self.fmodel.to(torch.float32)\n",
        "\n",
        "        self.fmodel.eval()\n",
        "\n",
        "    def grayscale_to_rgb(self, img, num_slices=256):\n",
        "        # Get shape of the images (inc batch size).\n",
        "        height, width, slices = 256,256, num_slices\n",
        "\n",
        "        # Hold RGB values.\n",
        "        rgb_image = np.zeros((height, width, slices, 3), dtype=np.float32)\n",
        "\n",
        "        # Repeat values for each channel.\n",
        "        for i in range(3):\n",
        "            rgb_image[..., i] = img.squeeze()\n",
        "\n",
        "        return rgb_image\n",
        "\n",
        "    def img_prob(self, input, batch):\n",
        "        #input = torch.from_numpy(input)\n",
        "        m = nn.Sigmoid()\n",
        "\n",
        "        #batch = 4\n",
        "\n",
        "        prob = np.zeros([batch,2])\n",
        "\n",
        "        for batch_idx in range (0, batch):\n",
        "\n",
        "          cur = input[batch_idx]\n",
        "          no = m(cur)\n",
        "\n",
        "          no = no.cpu().detach().numpy()\n",
        "\n",
        "          #rewrite to make it batches\n",
        "          total_pos_vals = (no>0.5).sum()\n",
        "          total_neg_vals = (no<0.5).sum()\n",
        "\n",
        "          if(total_pos_vals > 0):\n",
        "            pos_val_sum = no[no>0.5].sum()\n",
        "            pos_prob = pos_val_sum/total_pos_vals\n",
        "\n",
        "          else:\n",
        "            pos_prob = 0\n",
        "\n",
        "\n",
        "          if (total_neg_vals >0) :\n",
        "            neg_val_sum = total_neg_vals - (no[no<0.5].sum())\n",
        "            neg_prob = neg_val_sum/total_neg_vals\n",
        "\n",
        "          else:\n",
        "            neg_prob = 0\n",
        "\n",
        "\n",
        "\n",
        "          prob[batch_idx] = pos_prob,neg_prob\n",
        "        #print(prob)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input is 2, 1, 256, 256, 256\n",
        "        #batch, color, height, width, slice\n",
        "        x = x.cpu().detach().numpy().astype(np.float32)\n",
        "        batch, color, height, width, slice_num = x.shape\n",
        "        #results = np.zeros([batch, 0, height, width])\n",
        "        results = np.zeros((batch, slice_num, height, width), dtype=np.float32)\n",
        "        #print(results.shape)\n",
        "        self.fmodel.eval()\n",
        "        for batch_idx in range (0, batch):\n",
        "          #cur_x shape is 1,128,128,128 (color, height, width slice)\n",
        "          cur_x = x[batch_idx]\n",
        "\n",
        "          #cur_x shape is 128,128,128,3 (height, width, slices, color)\n",
        "          cur_x = self.grayscale_to_rgb(cur_x)\n",
        "          #print(x.shape)\n",
        "\n",
        "          #shape is now slice, color, height, width\n",
        "          cur_x = np.transpose(cur_x, (2,3,0,1)) # reorder dimensions to match input shape\n",
        "          #print(cur_x.shape)\n",
        "          tensor_img = torch.from_numpy(cur_x).to(self.device)\n",
        "\n",
        "\n",
        "          with torch.no_grad():\n",
        "              for y in range(0, slice_num):\n",
        "                  tensor_slice = tensor_img[y,:,:]\n",
        "\n",
        "                  tensor_slice = tensor_slice.to(torch.float32)\n",
        "                  # tensor_slice = tensor_slice.to(self.device)\n",
        "                  # tensor_slice = tensor_slice.type(torch.cuda.FloatTensor)\n",
        "                  img_t = torch.unsqueeze(tensor_slice, dim=0)\n",
        "                  #output = self.fmodel(img_t)[\"out\"]\n",
        "                  #print(img_t.shape)\n",
        "                  output = self.fmodel(img_t)[\"out\"]\n",
        "                  numpy_out = output.cpu().detach().numpy()[0][0]\n",
        "                  #numpy_out = np.array([numpy_out[0][0]])\n",
        "                  #print(numpy_out.shape)\n",
        "                  #print(results[batch_idx].shape)\n",
        "                  #results = np.append(results[batch_idx], numpy_out,0)\n",
        "                  results[batch_idx,y,:,:] = numpy_out\n",
        "                  torch.cuda.empty_cache()\n",
        "\n",
        "        #results = torch.from_numpy(results)\n",
        "        #results = torch.unsqueeze(results, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        resized_results = np.zeros((batch, slice_num, 128,128), dtype=np.float32)\n",
        "        for batch_idx in range (0, batch):\n",
        "          cur_x = results[batch_idx]\n",
        "          for x in range(0, slice_num):\n",
        "              cur_slice = cur_x[x,:,:]\n",
        "              #print(cur_slice.shape)\n",
        "              resize = cv2.resize(cur_slice, dsize = (128,128), interpolation = cv2.INTER_LINEAR)\n",
        "              resize = np.expand_dims(resize, axis=0) # add an extra dimension to resize\n",
        "              resized_results[batch_idx,y,:,:] = resize\n",
        "\n",
        "        resized_results = torch.from_numpy(resized_results)\n",
        "        resized_results = resized_results.to(torch.float32)\n",
        "        resized_results = resized_results.to(self.device)\n",
        "        #print(resized_results.shape)\n",
        "        #print(results.shape)\n",
        "\n",
        "        #print(resized_results.shape)\n",
        "        fin = self.img_prob(resized_results, batch = batch)\n",
        "        fin = torch.from_numpy(fin)\n",
        "        fin = fin.to(torch.float32)\n",
        "        fin = fin.to(self.device)\n",
        "        #print(fin.shape)\n",
        "        return fin\n",
        "\n",
        "\n",
        "  #code to load the model\n",
        "  #the process nii and run through mode\n",
        "  #if batching doesnt work on the processing nii, loop through each png induvidually and get a infrences\n",
        "  #to compile them create an empty numpy array which is the same shape as the large nii image, for each slice, put it in the slice into the correct location\n",
        "  #convert numpy to tensor and return"
      ],
      "metadata": {
        "id": "EzGUG2WGLs_2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mlp(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(mlp, self).__init__()\n",
        "        self.device = torch.device(\"cuda:0\")\n",
        "        PATH = '/content/drive/MyDrive/Science Fair 23-24/Pancreas stuff/MLP_0.8276.pt'\n",
        "        self.bmodel = torch.load(PATH)\n",
        "        self.bmodel.eval()\n",
        "\n",
        "    def forward(self, y):\n",
        "        output = self.bmodel(y)\n",
        "        return output"
      ],
      "metadata": {
        "id": "wR1gcDYZot7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C_bPw5PYMmY1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class multimodal(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(multimodal, self).__init__()\n",
        "        # Images\n",
        "        #BatchNorm = nn.InstanceNorm3d\n",
        "        #filters = [32, 64, 128, 256, 256, 512]\n",
        "        self.backbone = MyModel()\n",
        "        #self.backbone = AlignedXception(BatchNorm, filters)\n",
        "\n",
        "        # Descriptor\n",
        "        self.fc_d = nn.Linear(76, 2)  #change between 512 and 256\n",
        "\n",
        "        # Combination\n",
        "\n",
        "        #self._fc0 = nn.Linear(filters[-1] * 4 * 4 * 4 + 512, filters[-1])\n",
        "        #self._fc0 = nn.Linear( 513, 512) #256*128*128\n",
        "        #self._dropout = nn.Dropout(0.2)\n",
        "        #self._fc = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.my_fc = nn.Linear(4,2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Images\n",
        "        x = self.backbone(x) #takes image goes through backbone\n",
        "        #print(x.shape)\n",
        "\n",
        "\n",
        "        x = x.view(x.shape[0], -1) #flattening image?\n",
        "        #print(x.shape)\n",
        "\n",
        "        # Descriptor\n",
        "        y = self.sigmoid(self.fc_d(y)) #proccesses biomarker info\n",
        "        #print(y.get_device())\n",
        "        #print(y)\n",
        "        #print(y.dtype)\n",
        "        # Combination\n",
        "        #print(torch.cat([x,y], dim=1))\n",
        "        x = self.my_fc(torch.cat([x, y], dim=1)) #concatonates the biomarker with the image\n",
        "        #print(x)\n",
        "        #x = self._dropout(x) #dropout\n",
        "        #x = self._fc(x) #another fully connected\n",
        "\n",
        "        return x #gives output\n",
        "\n",
        "\n",
        "\n",
        "    #print(output.size())\n",
        "    #print(output)\n",
        "    #numpy_output = output.cpu().detach().numpy()\n",
        "\n",
        "    #plt.imshow(numpy_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class multimodalv2(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(multimodalv2, self).__init__()\n",
        "        # Images\n",
        "        #BatchNorm = nn.InstanceNorm3d\n",
        "        #filters = [32, 64, 128, 256, 256, 512]\n",
        "        self.backbone = MyModel()\n",
        "        self.biomarkers = mlp()\n",
        "\n",
        "        #self.backbone = AlignedXception(BatchNorm, filters)\n",
        "\n",
        "        # Descriptor\n",
        "        self.fc_d = nn.Linear(76, 2)  #change between 512 and 256\n",
        "\n",
        "        # Combination\n",
        "\n",
        "        #self._fc0 = nn.Linear(filters[-1] * 4 * 4 * 4 + 512, filters[-1])\n",
        "        #self._fc0 = nn.Linear( 513, 512) #256*128*128\n",
        "        #self._dropout = nn.Dropout(0.2)\n",
        "        #self._fc = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.my_fc = nn.Linear(4,2)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Images\n",
        "        x = self.backbone(x) #takes image goes through backbone\n",
        "        #print(x.shape)\n",
        "\n",
        "\n",
        "        x = x.view(x.shape[0], -1) #flattening image?\n",
        "        #print(x.shape)\n",
        "\n",
        "        # Descriptor\n",
        "        y = self.sigmoid(self.biomarkers(y))\n",
        "         #proccesses biomarker info\n",
        "        #print(y.get_device())\n",
        "        #print(y)\n",
        "        #print(y.dtype)\n",
        "        # Combination\n",
        "        #print(torch.cat([x,y], dim=1))\n",
        "        x = self.my_fc(torch.cat([x, y], dim=1)) #concatonates the biomarker with the image\n",
        "        #print(x)\n",
        "        #x = self._dropout(x) #dropout\n",
        "        #x = self._fc(x) #another fully connected\n",
        "\n",
        "        return x #gives output\n",
        "\n",
        "\n",
        "\n",
        "    #print(output.size())\n",
        "    #print(output)\n",
        "    #numpy_output = output.cpu().detach().numpy()\n",
        "\n",
        "    #plt.imshow(numpy_output)"
      ],
      "metadata": {
        "id": "gXjP1QstocH9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "8hKw_ADuFwTX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BCV-Uniandes/LUCAS\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/LUCAS')"
      ],
      "metadata": {
        "id": "XRkuXHJyF9IQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d482d9bc-7aa9-40c4-dcda-86e5dbd38f2b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LUCAS'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 10 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (40/40), 86.56 KiB | 805.00 KiB/s, done.\n",
            "Resolving deltas: 100% (8/8), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import util.utils as utils\n",
        "import util.Read_data as Read_data\n",
        "from util.save_graphs import save_graph\n",
        "#from modeling.model import DeepLab"
      ],
      "metadata": {
        "id": "S8eP5f0dF5lY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MRIdataset(Dataset):\n",
        "    def __init__(self, csv_file, descriptor, root_dir, image_size):\n",
        "        super(MRIdataset, self).__init__()\n",
        "        self.root_dir = root_dir + '/scans'\n",
        "        self.image_size = np.asarray(image_size)\n",
        "\n",
        "        self.task = -2  # -2=cancer; -1=nodule/mass\n",
        "\n",
        "        labels = pd.read_csv(csv_file)\n",
        "        self.labels = labels.set_index('patient_id').T.to_dict('list')\n",
        "        descriptor = pd.read_csv(descriptor)\n",
        "        self.descriptor = descriptor.set_index('patient_id').T.to_dict('list')\n",
        "        self.idx = list(self.labels.keys())\n",
        "\n",
        "\n",
        "        #sizes = pd.read_csv('../Data/sizes.csv')\n",
        "        #sizes = sizes.set_index('access').T.to_dict('list')\n",
        "        #for i in sizes:\n",
        "            #if int(sizes[i][0]) < 250 and i in self.idx:\n",
        "                #self.idx.remove(i)\n",
        "        self.weights = self.weights_balanced()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        patient = self.idx[idx]\n",
        "        label = self.labels[patient][self.task]\n",
        "        descriptor = self.descriptor[patient]\n",
        "        #descriptor = descriptor[1:12] + descriptor[13:-1]\n",
        "        descriptor = descriptor[1:-1]\n",
        "        descriptor = np.array(descriptor, dtype=np.float32)\n",
        "        #print(str(patient))\n",
        "        image = load_image(str(patient) + '.nii.gz', self.root_dir)\n",
        "\n",
        "        if any(np.asarray(image.shape) <= self.image_size):\n",
        "            dif = self.image_size - image.shape\n",
        "            mod = dif % 2\n",
        "            dif = dif // 2\n",
        "            pad = np.maximum(dif, [0, 0, 0])\n",
        "            pad = tuple(zip(pad, pad + mod))\n",
        "            image = np.pad(image, pad, 'reflect')\n",
        "\n",
        "        sz = self.image_size[0]\n",
        "        if any(np.asarray(image.shape) >= self.image_size):\n",
        "            x, y, z = image.shape\n",
        "            x = x // 2 - (sz // 2)\n",
        "            y = y // 2 - (sz // 2)\n",
        "            z = z // 2 - (sz // 2)\n",
        "            image = image[x:x + sz, y:y + sz, z:z + sz]\n",
        "        # Stats obtained from the MSD dataset\n",
        "        image = np.clip(image, a_min=-1024, a_max=326)\n",
        "        image = (image - 159.14433291523548) / 323.0573880113456\n",
        "\n",
        "        return {'data': np.expand_dims(image, 0),\n",
        "                'descriptor': descriptor, 'target': label, 'id':patient}\n",
        "\n",
        "    def weights_balanced(self):\n",
        "        count = [0] * 2\n",
        "        for item in self.idx:\n",
        "            count[self.labels[item][self.task]] += 1\n",
        "        weight_per_class = [0.] * 2\n",
        "        N = float(sum(count))\n",
        "        for i in range(2):\n",
        "            weight_per_class[i] = N/float(count[i])\n",
        "        weight = [0] * len(self.idx)\n",
        "        for idx, val in enumerate(self.idx):\n",
        "            weight[idx] = weight_per_class[self.labels[val][self.task]]\n",
        "        return weight\n",
        "\n",
        "\n",
        "class collate(object):\n",
        "    def __init__(self, size):\n",
        "        self.transforms = DA.Compose([\n",
        "            DA.NumpyToTensor()])\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        elem = batch[0]\n",
        "        batch = {key: np.stack([d[key] for d in batch]) for key in elem}\n",
        "        return self.transforms(**batch)\n",
        "\n",
        "\n",
        "def load_image(patient, root_dir):\n",
        "    im = nib.load(os.path.join(root_dir, patient))\n",
        "    image = im.get_fdata()\n",
        "    return image"
      ],
      "metadata": {
        "id": "h4ZfCN7FF2v0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "def main():\n",
        "    lr = 0.01\n",
        "    epochs = 15\n",
        "    patience = 3\n",
        "    batch = 4\n",
        "    gpu = '1'\n",
        "    training = True\n",
        "    #os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
        "    image_size = [256,256,256]\n",
        "    num_classes = 2\n",
        "    patience = 3\n",
        "\n",
        "    #fix\n",
        "    save_path = '/content/drive/MyDrive/Science Fair 23-24/Pancreas stuff/multimodal_log' #multimodal log folder          #os.path.join('TRAIN', args.name) #change\n",
        "\n",
        "    root = '/content/drive/MyDrive/Science fair 22-23/data/LUCAS'      # Root directory to the data\n",
        "    train_file = os.path.join(root, 'new.csv')\n",
        "    test_file = os.path.join(root, 'new_test.csv')\n",
        "    train_des = os.path.join(root, 'train_descriptor.csv')\n",
        "    test_des = '/content/drive/MyDrive/Science Fair 23-24/Pancreas stuff/test_descriptor.csv'   #os.path.join(root, 'test_descriptor.csv')\n",
        "\n",
        "\n",
        "    np.random.seed(12345)\n",
        "    torch.manual_seed(12345)\n",
        "    torch.cuda.manual_seed_all(12345)\n",
        "\n",
        "    model = multimodalv2(num_classes = num_classes)\n",
        "    print('---> Number of params: {}'.format(\n",
        "        sum([p.data.nelement() for p in model.parameters()])))\n",
        "\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr,\n",
        "                           weight_decay=1e-5, amsgrad=True)\n",
        "\n",
        "\n",
        "    annealing = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, patience = patience, verbose=True)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    #dataloaders\n",
        "    train_data = MRIdataset(train_file, train_des, root,\n",
        "                                      image_size)\n",
        "    test_data = MRIdataset(test_file, test_des, root,\n",
        "                                     image_size)\n",
        "\n",
        "    best_f1 = 0\n",
        "\n",
        "    #try without this\n",
        "    sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
        "        train_data.weights, len(train_data.weights))\n",
        "    train_loader = DataLoader(train_data, sampler=sampler,\n",
        "                              batch_size=batch, num_workers=2)\n",
        "    test_loader = DataLoader(test_data, shuffle=False, sampler=None,\n",
        "                             batch_size=batch, num_workers=2)\n",
        "\n",
        "\n",
        "    is_best = True\n",
        "    if training:\n",
        "        torch.cuda.empty_cache()\n",
        "        out_file = open(os.path.join(save_path, 'progress.csv'), 'a+')\n",
        "\n",
        "        for epoch in range(0, 15):       #epochs + 1, epochs + 1):  #double check the epoch formating and numbers mayb just hardquote\n",
        "            epochs = epoch\n",
        "            lr = utils.get_lr(optimizer)\n",
        "            print('--------- Starting Epoch {} --> {} ---------'.format(\n",
        "                epoch, time.strftime(\"%H:%M:%S\")))\n",
        "            print('Learning rate:', lr)\n",
        "\n",
        "\n",
        "            #figure out what args are going into here and are used\n",
        "            train_loss = train(model, train_loader, optimizer, bce)\n",
        "            test_loss, f1, flag = test(model, test_loader, save_path, bce)\n",
        "\n",
        "            out_file.write('{},{},{},{},{}\\n'.format(\n",
        "                epoch, train_loss, test_loss, f1, lr))\n",
        "            out_file.flush()\n",
        "\n",
        "            annealing.step(test_loss)\n",
        "            save_graph(save_path)\n",
        "\n",
        "            # To avoid saving as \"the best model\" one that always predict the\n",
        "            # same category\n",
        "            is_best = False\n",
        "            if flag:\n",
        "                is_best = best_f1 < f1\n",
        "                best_f1 = max(best_f1, f1)\n",
        "\n",
        "            state = {\n",
        "                'epoch': epoch,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'loss': [train_loss, test_loss],\n",
        "                'lr': lr,\n",
        "                'f1': f1,\n",
        "                'best_f1': best_f1}\n",
        "\n",
        "            checkpoint = epoch % 50 == 0\n",
        "            utils.save_epoch(state, save_path, epoch,\n",
        "                             checkpoint=checkpoint, is_best=is_best)\n",
        "\n",
        "            if lr <= (lr / (10 ** 4)):\n",
        "                print('Stopping training: learning rate is too small')\n",
        "                break\n",
        "        out_file.close()\n",
        "\n",
        "\n",
        "    #figure out what args are needed here and change that\n",
        "    val_loss, flag = test(model, test_loader, save_path, bce, False)\n",
        "    save_graph(save_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KpQi9nA3umrO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, bce):\n",
        "    model.train()\n",
        "    epoch_loss = utils.AverageMeter()\n",
        "    batch_loss = utils.AverageMeter()\n",
        "\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "    print_stats = len(loader) // 5\n",
        "    for batch_idx, sample in enumerate(loader):\n",
        "        data = sample['data'].float().to(device = device, dtype = torch.float32)\n",
        "        descriptor = sample['descriptor'].to(device)\n",
        "        target = sample['target'].float().to(device)\n",
        "        target = torch.stack([1 - target, target], dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data, descriptor)\n",
        "        loss = bce(out, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss.update(loss.item())\n",
        "        epoch_loss.update(loss.item())\n",
        "\n",
        "        if batch_loss.count % print_stats == 0:\n",
        "            text = '{} -- [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'\n",
        "            print(text.format(\n",
        "                time.strftime(\"%H:%M:%S\"), (batch_idx + 1),\n",
        "                (len(loader)), 100. * (batch_idx + 1) / (len(loader)),\n",
        "                batch_loss.avg))\n",
        "            batch_loss.reset()\n",
        "    print('--- Train: \\tLoss: {:.6f} ---'.format(epoch_loss.avg))\n",
        "    return epoch_loss.avg\n",
        "\n",
        "\n",
        "def test(model, loader, save_path, bce, training=False):\n",
        "    model.eval()\n",
        "    epoch_loss = utils.AverageMeter()\n",
        "    count, correct = 0, 0\n",
        "    labels, patients, scores, predictions = [], [], [], []\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "\n",
        "    for batch_idx, sample in enumerate(loader):\n",
        "        data = sample['data'].float().to(device)\n",
        "        descriptor = sample['descriptor'].float().to(device)\n",
        "        target = sample['target'].float().to(device)\n",
        "        patients.extend(sample['id'].tolist())\n",
        "        labels.extend(sample['target'].tolist())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(data, descriptor)\n",
        "        loss = bce(out, torch.stack([1 - target, target], dim=1))\n",
        "        epoch_loss.update(loss.item())\n",
        "\n",
        "        m = nn.Sigmoid()\n",
        "\n",
        "\n",
        "        confidence = F.softmax(out, dim =1)       #m(out) #mayb switch back to softmax\n",
        "        #print(confidence.size())\n",
        "        #print(confidence.size)\n",
        "        scores.extend(confidence[:, 1].tolist())\n",
        "\n",
        "        pred = torch.argmax(confidence, dim=1)\n",
        "        predictions.extend(pred.tolist())   #THIS LINE IS BROKEN???\n",
        "        count += pred.sum()\n",
        "        correct += (pred * target).sum()\n",
        "\n",
        "    print('--- Val: \\tLoss: {:.6f} ---'.format(epoch_loss.avg))\n",
        "\n",
        "    # Metrics\n",
        "    roc = roc_auc_score(labels, scores)\n",
        "    ap = average_precision_score(labels, scores)\n",
        "    f1 = f1_score(labels, predictions)\n",
        "\n",
        "    if not training:\n",
        "        print('ROC', roc, 'AP', ap, 'F1', f1)\n",
        "        rows = zip(patients, scores)\n",
        "        with open(os.path.join(save_path, 'confidence.csv'), \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['ROC:', roc])\n",
        "            writer.writerow(['AP:', ap])\n",
        "            writer.writerow(['F1:', f1])\n",
        "            for row in rows:\n",
        "                writer.writerow(row)\n",
        "\n",
        "    count = count.sum()\n",
        "    flag = True\n",
        "    if count == 0 or count == len(loader.dataset):\n",
        "        flag = False\n",
        "    return epoch_loss.avg, f1, flag"
      ],
      "metadata": {
        "id": "6HFgA0Og5w_W"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "msroCzlgccZo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "xYdcb6O26Z_T",
        "outputId": "4904aef3-148a-4627-8082-5f467e5a47a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c7bc734e5e35>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-ce4b887e07ce>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12345\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultimodalv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     print('---> Number of params: {}'.format(\n\u001b[1;32m     30\u001b[0m         sum([p.data.nelement() for p in model.parameters()])))\n",
            "\u001b[0;32m<ipython-input-7-04b22013d67c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m#filters = [32, 64, 128, 256, 256, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiomarkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#self.backbone = AlignedXception(BatchNorm, filters)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-6a6156ed2cfd>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Science Fair 23-24/Pancreas stuff/MLP_0.8276.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;31m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'MLP' on <module '__main__'>"
          ]
        }
      ]
    }
  ]
}